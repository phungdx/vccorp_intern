{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyspark","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-09T08:13:19.776815Z","iopub.execute_input":"2022-05-09T08:13:19.777433Z","iopub.status.idle":"2022-05-09T08:14:19.385593Z","shell.execute_reply.started":"2022-05-09T08:13:19.777336Z","shell.execute_reply":"2022-05-09T08:14:19.384778Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install findspark","metadata":{"execution":{"iopub.status.busy":"2022-05-09T08:14:21.260761Z","iopub.execute_input":"2022-05-09T08:14:21.261098Z","iopub.status.idle":"2022-05-09T08:14:32.535947Z","shell.execute_reply.started":"2022-05-09T08:14:21.261062Z","shell.execute_reply":"2022-05-09T08:14:32.534813Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import findspark\nfindspark.init","metadata":{"execution":{"iopub.status.busy":"2022-05-09T08:14:35.470342Z","iopub.execute_input":"2022-05-09T08:14:35.470663Z","iopub.status.idle":"2022-05-09T08:14:35.484396Z","shell.execute_reply.started":"2022-05-09T08:14:35.470629Z","shell.execute_reply":"2022-05-09T08:14:35.483419Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql import SparkSession # required to created a dataframe\nspark = SparkSession.builder.appName(\"Basics\").getOrCreate()\n\nimport pyspark.sql.functions as F","metadata":{"execution":{"iopub.status.busy":"2022-05-09T08:14:42.532289Z","iopub.execute_input":"2022-05-09T08:14:42.532545Z","iopub.status.idle":"2022-05-09T08:14:48.299041Z","shell.execute_reply.started":"2022-05-09T08:14:42.532518Z","shell.execute_reply":"2022-05-09T08:14:48.298074Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df = spark.read.csv(\"../input/heart-failure-prediction/heart.csv\",\n                    header=True,\n                   inferSchema=True)\n\n# read csv, all columns will be of type string\n# df = spark.read.option('header','true').csv('heart.csv')\n# tell pyspark the type of the columns - saves time on large dataset. there are other ways to do this, but that's\n# my favorite\n# schema = 'Age INTEGER, Sex STRING, ChestPainType STRING'\n# df = spark.read.csv('../input/heart-failure-prediction/heart.csv', schema=schema, inferSchema=True ,header=True)\n# let PySpark infer the schema\n# df = spark.read.csv('../input/heart-failure-prediction/heart.csv', inferSchema=True, header=True)\n# # replace nulls with other value at reading time\n# df = spark.read.csv('../input/heart-failure-prediction/heart.csv', nullValue='NA')\n# save data\n# df.write.format(\"csv\").save(\"./heart_save.csv\")\n# # if you want to overwrite the file\n# df.write.format(\"csv\").mode(\"overwrite\").save(\"./heart_save.csv\")\n\ndf.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-09T09:06:29.084981Z","iopub.execute_input":"2022-05-09T09:06:29.085321Z","iopub.status.idle":"2022-05-09T09:06:29.669697Z","shell.execute_reply.started":"2022-05-09T09:06:29.085287Z","shell.execute_reply":"2022-05-09T09:06:29.668712Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"df.printSchema()","metadata":{"execution":{"iopub.status.busy":"2022-05-09T09:06:36.389823Z","iopub.execute_input":"2022-05-09T09:06:36.390105Z","iopub.status.idle":"2022-05-09T09:06:36.395882Z","shell.execute_reply.started":"2022-05-09T09:06:36.390076Z","shell.execute_reply":"2022-05-09T09:06:36.395215Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"df.count()","metadata":{"execution":{"iopub.status.busy":"2022-05-09T09:06:39.504821Z","iopub.execute_input":"2022-05-09T09:06:39.505145Z","iopub.status.idle":"2022-05-09T09:06:39.642713Z","shell.execute_reply.started":"2022-05-09T09:06:39.505113Z","shell.execute_reply":"2022-05-09T09:06:39.641867Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"# Pandas DataFrame VS PySpark DataFrame","metadata":{}},{"cell_type":"markdown","source":"**both represents a table of data with rows and columns. however, under the hood they are different, as PySpark dataframe needs to support distributed computations. as we move forward, we will see more and more features of it that are not present in Pandas DataFrame. that being said - if you know how to use Pandas, than moving to PySpark will feel like a natural transition.**","metadata":{}},{"cell_type":"markdown","source":"# DAG","metadata":{}},{"cell_type":"markdown","source":"**directed acyclic graph is the way Spark runs computations. when you give it a series of transformation to apply to the dataset, it build a graph out of those transformations, so it knows what to do - but it does not execute those commands immediately, if it does not have to. rather, it is lazy - it will go through the DAG and apply the transformations only when it must, to provide a needed result. this allows better performance, since spark knows what's ahead of a certain computation and get optimize the process accordingly.**","metadata":{}},{"cell_type":"markdown","source":"# transformations VS actions","metadata":{}},{"cell_type":"markdown","source":"**in PySpark, there are two types of command: transformations and actions. transformation commands are added to the DAG, but does not get it to actually be executed. they transform one DataFrame into another, not changing the input DataFrame. on the other hand, actions make PySpark execute the DAG but does not create a new DataFrame - instead, they output the result of the DAG.**","metadata":{}},{"cell_type":"markdown","source":"# Caching","metadata":{}},{"cell_type":"markdown","source":"**every time you run a DAG, it will be re-computed from the beginning. that is, the results are not saved in memory. so, if we want to save a result so it won't have to be recomputed, we can use the cache command. note, that this will occupy space in the working node's memory - so be careful with the sizes of datasets you are caching! by default, the cached DF is stored to RAM, and is unserialized (not converted into a stream of bytes). you can change both of these - store data to hard disk, serialized it, or both!**","metadata":{}},{"cell_type":"markdown","source":"# Collecting","metadata":{}},{"cell_type":"markdown","source":"**even after caching a DataFrame, it still sits in the worker nodes memory. if you want to collect its pieces, assemble them and save them on the master node so you won't have to pull it every time, use the command for collecting. again, be very careful with this, since the collected file will have to fit in the master node memory!**","metadata":{}},{"cell_type":"code","source":"df.cache()\ndf.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-09T09:06:47.491517Z","iopub.execute_input":"2022-05-09T09:06:47.491824Z","iopub.status.idle":"2022-05-09T09:06:47.643741Z","shell.execute_reply.started":"2022-05-09T09:06:47.491791Z","shell.execute_reply":"2022-05-09T09:06:47.634436Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# convert PySpark DataFrame to Pandas DataFrame\npd_df = df.toPandas()\n# convert it back\nspark_df = spark.createDataFrame(pd_df)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T09:07:00.887650Z","iopub.execute_input":"2022-05-09T09:07:00.887929Z","iopub.status.idle":"2022-05-09T09:07:01.083622Z","shell.execute_reply.started":"2022-05-09T09:07:00.887902Z","shell.execute_reply":"2022-05-09T09:07:01.082847Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# show first three rows as three row objects, which is how spark represents single rows from a table.\n# we will learn more about it later\ndf.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T09:07:03.775982Z","iopub.execute_input":"2022-05-09T09:07:03.776868Z","iopub.status.idle":"2022-05-09T09:07:03.833201Z","shell.execute_reply.started":"2022-05-09T09:07:03.776817Z","shell.execute_reply":"2022-05-09T09:07:03.832335Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-05-09T09:07:23.504456Z","iopub.execute_input":"2022-05-09T09:07:23.504759Z","iopub.status.idle":"2022-05-09T09:07:23.511963Z","shell.execute_reply.started":"2022-05-09T09:07:23.504726Z","shell.execute_reply":"2022-05-09T09:07:23.511078Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# cast a column from one type to other\nfrom pyspark.sql.types import FloatType\ndf = df.withColumn(\"Age\",df.Age.cast(FloatType()))\ndf = df.withColumn(\"RestingBP\",df.Age.cast(FloatType()))","metadata":{"execution":{"iopub.status.busy":"2022-05-09T09:07:30.971112Z","iopub.execute_input":"2022-05-09T09:07:30.971413Z","iopub.status.idle":"2022-05-09T09:07:31.009728Z","shell.execute_reply.started":"2022-05-09T09:07:30.971386Z","shell.execute_reply":"2022-05-09T09:07:31.008832Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# compute summery statistics\ndf.select(['Age','RestingBP']).describe().show()","metadata":{"execution":{"iopub.status.busy":"2022-05-09T09:07:51.159728Z","iopub.execute_input":"2022-05-09T09:07:51.160001Z","iopub.status.idle":"2022-05-09T09:07:51.630146Z","shell.execute_reply.started":"2022-05-09T09:07:51.159972Z","shell.execute_reply":"2022-05-09T09:07:51.629104Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# add a new column or replace existing one\nAgeFixed = df['Age'] + 1  # select alwayes returns a DataFrame object, and we need a column object\ndf = df.withColumn('AgeFixed', AgeFixed)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T09:12:31.403237Z","iopub.execute_input":"2022-05-09T09:12:31.403560Z","iopub.status.idle":"2022-05-09T09:12:31.480393Z","shell.execute_reply.started":"2022-05-09T09:12:31.403531Z","shell.execute_reply":"2022-05-09T09:12:31.479621Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"df.select(['AgeFixed','Age']).describe().show()","metadata":{"execution":{"iopub.status.busy":"2022-05-09T09:12:43.440009Z","iopub.execute_input":"2022-05-09T09:12:43.440324Z","iopub.status.idle":"2022-05-09T09:12:43.713162Z","shell.execute_reply.started":"2022-05-09T09:12:43.440275Z","shell.execute_reply":"2022-05-09T09:12:43.712068Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# remove columns\ndf.drop('AgeFixed').show(1) # add df = to get the new DataFrame into a variable","metadata":{"execution":{"iopub.status.busy":"2022-05-09T09:12:58.592712Z","iopub.execute_input":"2022-05-09T09:12:58.593040Z","iopub.status.idle":"2022-05-09T09:12:58.719210Z","shell.execute_reply.started":"2022-05-09T09:12:58.592993Z","shell.execute_reply":"2022-05-09T09:12:58.718010Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# rename a column\n# df.withColumnRenamed('Age','age').select('age').show(1)\n# to rename more than a single column, i would suggest a loop.\nname_pairs = [('Age','age'),('Sex','sex')]\nfor old_name, new_name in name_pairs:\n    df = df.withColumnRenamed(old_name,new_name)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T09:16:49.469090Z","iopub.execute_input":"2022-05-09T09:16:49.469345Z","iopub.status.idle":"2022-05-09T09:16:49.489210Z","shell.execute_reply.started":"2022-05-09T09:16:49.469318Z","shell.execute_reply":"2022-05-09T09:16:49.488335Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"df.show(1)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T09:16:56.819851Z","iopub.execute_input":"2022-05-09T09:16:56.820130Z","iopub.status.idle":"2022-05-09T09:16:56.909698Z","shell.execute_reply.started":"2022-05-09T09:16:56.820101Z","shell.execute_reply":"2022-05-09T09:16:56.909075Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# drop all rows that contain any NA\ndf = df.na.drop()\ndf.count()\n# drop all rows where all values are NA\ndf = df.na.drop(how='all')\n# drop all rows where more at least 2 values are NOT NA\ndf = df.na.drop(thresh=2)\n# drop all rows where any value at specific columns are NAs.\ndf = df.na.drop(how='any', subset=['age','sex']) # 'any' is the defult","metadata":{"execution":{"iopub.status.busy":"2022-05-09T09:21:01.110069Z","iopub.execute_input":"2022-05-09T09:21:01.110399Z","iopub.status.idle":"2022-05-09T09:21:01.317473Z","shell.execute_reply.started":"2022-05-09T09:21:01.110356Z","shell.execute_reply":"2022-05-09T09:21:01.316523Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"# fill missing values in a specific column with a '?'\ndf = df.na.fill(value='?',subset=['sex'])\n# replace NAs with mean of column\nfrom pyspark.ml.feature import Imputer # In statistics, imputation is the process of\n                                       # replacing missing data with substituted values\nimptr = Imputer(inputCols=['age','RestingBP'],\n                outputCols=['age','RestingBP']).setStrategy('mean') # can also be 'median' and so on\n\ndf = imptr.fit(df).transform(df)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T09:23:17.633652Z","iopub.execute_input":"2022-05-09T09:23:17.633975Z","iopub.status.idle":"2022-05-09T09:23:18.566706Z","shell.execute_reply.started":"2022-05-09T09:23:17.633945Z","shell.execute_reply":"2022-05-09T09:23:18.565820Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"# filter to adults only and calculate mean\ndf.filter('age > 18').select('age').describe().show()\ndf.where('age > 18')# 'where' is an alias to 'filter'\ndf.where(df['age'] > 18) # third option\n# add another condition ('&' means and, '|' means or)\ndf.where((df['age'] > 18) | (df['ChestPainType'] == 'ATA'))\n# take every record where the 'ChestPainType' is NOT 'ATA'\ndf.filter(~(df['ChestPainType'] == 'ATA'))","metadata":{"execution":{"iopub.status.busy":"2022-05-09T09:26:59.438363Z","iopub.execute_input":"2022-05-09T09:26:59.438654Z","iopub.status.idle":"2022-05-09T09:26:59.734261Z","shell.execute_reply.started":"2022-05-09T09:26:59.438626Z","shell.execute_reply":"2022-05-09T09:26:59.733380Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"# evaluate a string expression into command\nfrom pyspark.sql.functions import expr\nexp = 'age + 0.2 * AgeFixed'\ndf.withColumn('new_col', expr(exp)).select('new_col').show(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T09:31:30.222969Z","iopub.execute_input":"2022-05-09T09:31:30.223317Z","iopub.status.idle":"2022-05-09T09:31:30.408360Z","shell.execute_reply.started":"2022-05-09T09:31:30.223284Z","shell.execute_reply":"2022-05-09T09:31:30.407419Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"# group by age\ndisease_by_age = df.groupby('age').mean().select(['age','avg(HeartDisease)'])\n# sort values in desnding order\nfrom pyspark.sql.functions import desc\ndisease_by_age.orderBy(desc(\"age\")).show(5)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T09:38:49.027429Z","iopub.execute_input":"2022-05-09T09:38:49.027745Z","iopub.status.idle":"2022-05-09T09:38:49.372516Z","shell.execute_reply.started":"2022-05-09T09:38:49.027716Z","shell.execute_reply":"2022-05-09T09:38:49.371589Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"# aggregate to get several statistics for several columns\n# the available aggregate functions are avg, max, min, sum, count\nfrom pyspark.sql import functions as F\ndf.agg(F.min(df['age']),F.max(df['age']),F.avg(df['sex'])).show()","metadata":{"execution":{"iopub.status.busy":"2022-05-09T09:40:48.576383Z","iopub.execute_input":"2022-05-09T09:40:48.576670Z","iopub.status.idle":"2022-05-09T09:40:48.867403Z","shell.execute_reply.started":"2022-05-09T09:40:48.576643Z","shell.execute_reply":"2022-05-09T09:40:48.866446Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"df.groupby('HeartDisease').agg(F.min(df['age']),F.avg(df['sex'])).show()","metadata":{"execution":{"iopub.status.busy":"2022-05-09T09:42:14.854637Z","iopub.execute_input":"2022-05-09T09:42:14.854933Z","iopub.status.idle":"2022-05-09T09:42:15.298951Z","shell.execute_reply.started":"2022-05-09T09:42:14.854904Z","shell.execute_reply":"2022-05-09T09:42:15.298090Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"# run an SQL query on the data\ndf.createOrReplaceTempView(\"df\") # tell PySpark how the table will be called in the SQL query\nspark.sql(\"\"\"SELECT sex from df\"\"\").show(2)\n\n# we also choose columns using SQL sytnx, with a command that combins '.select()' and '.sql()'\ndf.selectExpr(\"age >= 40 as older\", \"age\").show(2)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T09:50:51.115252Z","iopub.execute_input":"2022-05-09T09:50:51.116269Z","iopub.status.idle":"2022-05-09T09:50:51.426423Z","shell.execute_reply.started":"2022-05-09T09:50:51.116219Z","shell.execute_reply":"2022-05-09T09:50:51.425497Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"df.groupby('age').pivot('sex', (\"M\", \"F\")).count().show(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T10:10:39.617565Z","iopub.execute_input":"2022-05-09T10:10:39.617888Z","iopub.status.idle":"2022-05-09T10:10:40.068676Z","shell.execute_reply.started":"2022-05-09T10:10:39.617855Z","shell.execute_reply":"2022-05-09T10:10:40.067801Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"# pivot - expensive operation\ndf.selectExpr(\"age >= 40 as older\", \"age\",'sex').groupBy(\"sex\")\\\n                    .pivot(\"older\", (\"true\", \"false\")).count().show()","metadata":{"execution":{"iopub.status.busy":"2022-05-09T10:20:21.278162Z","iopub.execute_input":"2022-05-09T10:20:21.278486Z","iopub.status.idle":"2022-05-09T10:20:21.616434Z","shell.execute_reply.started":"2022-05-09T10:20:21.278456Z","shell.execute_reply":"2022-05-09T10:20:21.615660Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"df.select(['age','MaxHR','Cholesterol']).show(4)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T10:21:03.105663Z","iopub.execute_input":"2022-05-09T10:21:03.105968Z","iopub.status.idle":"2022-05-09T10:21:03.226951Z","shell.execute_reply.started":"2022-05-09T10:21:03.105937Z","shell.execute_reply":"2022-05-09T10:21:03.226084Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"df.printSchema()","metadata":{"execution":{"iopub.status.busy":"2022-05-09T10:25:19.582407Z","iopub.execute_input":"2022-05-09T10:25:19.582864Z","iopub.status.idle":"2022-05-09T10:25:19.588067Z","shell.execute_reply.started":"2022-05-09T10:25:19.582813Z","shell.execute_reply":"2022-05-09T10:25:19.587372Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"# devide dataset to training features and target\nX_column_names = ['age','Cholesterol']\ntarget_colum_name = ['MaxHR']\n\n# convert feature columns into a columns where the vlues are feature vectors\nfrom pyspark.ml.feature import VectorAssembler\nv_asmblr = VectorAssembler(inputCols = X_column_names, outputCol = 'Fvec')\ndf = v_asmblr.transform(df)\nX = df.select(['age','Cholesterol','Fvec','MaxHR'])\nX.show(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T10:29:42.137885Z","iopub.execute_input":"2022-05-09T10:29:42.138187Z","iopub.status.idle":"2022-05-09T10:29:42.686073Z","shell.execute_reply.started":"2022-05-09T10:29:42.138157Z","shell.execute_reply":"2022-05-09T10:29:42.685446Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"# devide dataset into training and testing sets\ntrainset, testset = X.randomSplit([0.8,0.2])","metadata":{"execution":{"iopub.status.busy":"2022-05-09T10:30:48.844467Z","iopub.execute_input":"2022-05-09T10:30:48.845006Z","iopub.status.idle":"2022-05-09T10:30:48.867806Z","shell.execute_reply.started":"2022-05-09T10:30:48.844965Z","shell.execute_reply":"2022-05-09T10:30:48.867164Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"# predict 'RestingBP' using linear regression\nfrom pyspark.ml.regression import LinearRegression\nmodel = LinearRegression(featuresCol='Fvec', labelCol='MaxHR')\nmodel = model.fit(trainset)\nprint(model.coefficients)\nprint(model.intercept)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T10:31:27.025981Z","iopub.execute_input":"2022-05-09T10:31:27.026793Z","iopub.status.idle":"2022-05-09T10:31:28.193315Z","shell.execute_reply.started":"2022-05-09T10:31:27.026750Z","shell.execute_reply":"2022-05-09T10:31:28.192581Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"# evaluate model\nmodel.evaluate(testset).predictions.show(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T10:31:41.013480Z","iopub.execute_input":"2022-05-09T10:31:41.014071Z","iopub.status.idle":"2022-05-09T10:31:41.431552Z","shell.execute_reply.started":"2022-05-09T10:31:41.014028Z","shell.execute_reply":"2022-05-09T10:31:41.430630Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"# handel categorical features with ordinal indexing\nfrom pyspark.ml.feature import StringIndexer\nindxr = StringIndexer(inputCol='ChestPainType', outputCol='ChestPainTypeInxed')\nindxr.fit(df).transform(df).select('ChestPainTypeInxed').show(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T10:36:41.113948Z","iopub.execute_input":"2022-05-09T10:36:41.114598Z","iopub.status.idle":"2022-05-09T10:36:41.542315Z","shell.execute_reply.started":"2022-05-09T10:36:41.114549Z","shell.execute_reply":"2022-05-09T10:36:41.541400Z"},"trusted":true},"execution_count":111,"outputs":[]}]}